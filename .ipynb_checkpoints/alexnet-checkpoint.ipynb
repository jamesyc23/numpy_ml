{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "3ef47c31-154d-43a8-9a9c-07fc146e0a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import (\n",
    "    Any,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Optional,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa1b252-1250-41ca-8a06-c4c426a51d63",
   "metadata": {},
   "source": [
    "## Defining the Tensor and Recipe classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3fc6a1c-6678-4fe8-b1cc-43a61bfa6d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Recipe:\n",
    "    func: Callable\n",
    "    args: tuple\n",
    "    kwargs: Dict[str, Any]\n",
    "    parents: Dict[int, Tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c90bdc7c-5444-4818-8be5-e64d52ab926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Arr = np.ndarray\n",
    "\n",
    "class Tensor:\n",
    "    '''\n",
    "    A drop-in replacement for torch.Tensor supporting a subset of features.\n",
    "    '''\n",
    "\n",
    "    array: Arr\n",
    "    \"The underlying array. Can be shared between multiple Tensors.\"\n",
    "    requires_grad: bool\n",
    "    \"If True, calling functions or methods on this tensor will track relevant data for backprop.\"\n",
    "    grad: Optional[\"Tensor\"]\n",
    "    \"Backpropagation will accumulate gradients into this field.\"\n",
    "    recipe: Optional[Recipe]\n",
    "    \"Extra information necessary to run backpropagation.\"\n",
    "\n",
    "    def __init__(self, array: Union[Arr, list], requires_grad=False):\n",
    "        self.array = array if isinstance(array, Arr) else np.array(array)\n",
    "        if self.array.dtype == np.float64:\n",
    "            self.array = self.array.astype(np.float32)\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad = np.zeros_like(self.array).astype(np.float32)\n",
    "        self.recipe = None\n",
    "        \"If not None, this tensor's array was created via recipe.func(*recipe.args, **recipe.kwargs).\"\n",
    "\n",
    "    def __neg__(self) -> \"Tensor\":\n",
    "        return negative(self)\n",
    "\n",
    "    def __add__(self, other) -> \"Tensor\":\n",
    "        return add(self, other)\n",
    "\n",
    "    def __radd__(self, other) -> \"Tensor\":\n",
    "        return add(other, self)\n",
    "\n",
    "    def __sub__(self, other) -> \"Tensor\":\n",
    "        return subtract(self, other)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return subtract(other, self)\n",
    "\n",
    "    def __mul__(self, other) -> \"Tensor\":\n",
    "        return multiply(self, other)\n",
    "\n",
    "    def __rmul__(self, other) -> \"Tensor\":\n",
    "        return multiply(other, self)\n",
    "\n",
    "    def __truediv__(self, other) -> \"Tensor\":\n",
    "        return true_divide(self, other)\n",
    "\n",
    "    def __rtruediv__(self, other) -> \"Tensor\":\n",
    "        return true_divide(other, self)\n",
    "\n",
    "    def __matmul__(self, other) -> \"Tensor\":\n",
    "        return matmul(self, other)\n",
    "\n",
    "    def __rmatmul__(self, other) -> \"Tensor\":\n",
    "        return matmul(other, self)\n",
    "\n",
    "    def __eq__(self, other) -> \"Tensor\":\n",
    "        return eq(self, other)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Tensor({repr(self.array)}, requires_grad={self.requires_grad})\"\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        if self.array.ndim == 0:\n",
    "            raise TypeError\n",
    "        return self.array.shape[0]\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return id(self)\n",
    "\n",
    "    def __getitem__(self, index) -> \"Tensor\":\n",
    "        return getitem(self, index)\n",
    "\n",
    "    def add_(self, other: \"Tensor\", alpha: float = 1.0) -> \"Tensor\":\n",
    "        add_(self, other, alpha=alpha)\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def T(self) -> \"Tensor\":\n",
    "        return permute(self, axes=(-1, -2))\n",
    "\n",
    "    def item(self):\n",
    "        return self.array.item()\n",
    "\n",
    "    def sum(self, dim=None, keepdim=False):\n",
    "        return sum(self, dim=dim, keepdim=keepdim)\n",
    "\n",
    "    def log(self):\n",
    "        return log(self)\n",
    "\n",
    "    def exp(self):\n",
    "        return exp(self)\n",
    "\n",
    "    def reshape(self, new_shape):\n",
    "        return reshape(self, new_shape)\n",
    "\n",
    "    def expand(self, new_shape):\n",
    "        return expand(self, new_shape)\n",
    "\n",
    "    def permute(self, dims):\n",
    "        return permute(self, dims)\n",
    "\n",
    "    def maximum(self, other):\n",
    "        return maximum(self, other)\n",
    "\n",
    "    def relu(self):\n",
    "        return relu(self)\n",
    "\n",
    "    def argmax(self, dim=None, keepdim=False):\n",
    "        return argmax(self, dim=dim, keepdim=keepdim)\n",
    "\n",
    "    def uniform_(self, low: float, high: float) -> \"Tensor\":\n",
    "        self.array[:] = np.random.uniform(low, high, self.array.shape)\n",
    "        return self\n",
    "\n",
    "    def backward(self, end_grad: Union[Arr, \"Tensor\", None] = None) -> None:\n",
    "        if isinstance(end_grad, Arr):\n",
    "            end_grad = Tensor(end_grad)\n",
    "        return backprop(self, end_grad)\n",
    "\n",
    "    def size(self, dim: Optional[int] = None):\n",
    "        if dim is None:\n",
    "            return self.shape\n",
    "        return self.shape[dim]\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.array.shape\n",
    "\n",
    "    @property\n",
    "    def ndim(self):\n",
    "        return self.array.ndim\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        '''Same as https://pytorch.org/docs/stable/generated/torch.Tensor.is_leaf.html'''\n",
    "        if self.requires_grad and self.recipe and self.recipe.parents:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def __bool__(self):\n",
    "        if np.array(self.shape).prod() != 1:\n",
    "            raise RuntimeError(\"bool value of Tensor with more than one value is ambiguous\")\n",
    "        return bool(self.item())\n",
    "\n",
    "def empty(*shape: int) -> Tensor:\n",
    "    '''Like torch.empty.'''\n",
    "    return Tensor(np.empty(shape))\n",
    "\n",
    "def zeros(*shape: int) -> Tensor:\n",
    "    '''Like torch.zeros.'''\n",
    "    return Tensor(np.zeros(shape))\n",
    "\n",
    "def arange(start: int, end: int, step=1) -> Tensor:\n",
    "    '''Like torch.arange(start, end).'''\n",
    "    return Tensor(np.arange(start, end, step=step))\n",
    "\n",
    "def tensor(array: Arr, requires_grad=False) -> Tensor:\n",
    "    '''Like torch.tensor.'''\n",
    "    return Tensor(array, requires_grad=requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2c2312-0a30-422b-b2df-6373b4d3cdfc",
   "metadata": {},
   "source": [
    "## Forward and Backward Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "0b4cf5e0-a41d-4348-9027-a404e95b0567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement broadcasting and unbroadcasting?\n",
    "def log_back(grad_out, out, x):\n",
    "    \"\"\"Returns the gradient of the loss w.r.t. x via out = log(x).\"\"\"\n",
    "    return grad_out / x\n",
    "\n",
    "def add_back0(grad_out, out, a, b):\n",
    "    \"\"\"Returns the gradient of the loss w.r.t. a via out = a + b, where + is the element-wise addition.\"\"\"\n",
    "    return grad_out\n",
    "\n",
    "def add_back1(grad_out, out, a, b):\n",
    "    \"\"\"Returns the gradient of the loss w.r.t. b via out = a + b, where + is element-wise addition.\"\"\"\n",
    "    return grad_out\n",
    "\n",
    "def multiply_back0(grad_out, out, a, b):\n",
    "    \"\"\"Returns the gradient of the loss w.r.t. a via out = a * b, where * is element-wise multiplication.\"\"\"\n",
    "    return grad_out * b\n",
    "\n",
    "def multiply_back1(grad_out, out, a, b):\n",
    "    \"\"\"Returns the gradient of the loss w.r.t. b via out = a * b, where * is element-wise multiplication.\"\"\"\n",
    "    return grad_out * a\n",
    "\n",
    "def true_divide_back0(grad_out, out, a, b):\n",
    "    return grad_out / b\n",
    "\n",
    "def true_divide_back1(grad_out, out, a, b):\n",
    "    return -grad_out * a / (b ** 2)\n",
    "\n",
    "# TODO: implement maximum\n",
    "def maximum_back0(grad_out, out, a, b):\n",
    "    pass\n",
    "\n",
    "def maximum_back1(grad_out, out, a, b):\n",
    "    pass\n",
    "\n",
    "def sum_back(grad_out, out, x, axis=None, keepdims=False):\n",
    "    if not isinstance(grad_out, Arr):\n",
    "        grad_out = np.array(grad_out)\n",
    "\n",
    "    if axis is None:\n",
    "        axis = list(range(x.ndim))\n",
    "\n",
    "    if keepdims == False:\n",
    "        print(grad_out, axis)\n",
    "        grad_out = np.expand_dims(grad_out, axis)\n",
    "\n",
    "    return np.broadcast_to(grad_out, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "3691147c-10cf-485e-bbd8-6b5403e6594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "BACK_FUNCS = defaultdict(dict)\n",
    "\n",
    "BACK_FUNCS[np.log][0] = log_back\n",
    "BACK_FUNCS[np.add][0] = add_back0\n",
    "BACK_FUNCS[np.add][1] = add_back1\n",
    "BACK_FUNCS[np.multiply][0] = multiply_back0\n",
    "BACK_FUNCS[np.multiply][1] = multiply_back1\n",
    "BACK_FUNCS[np.true_divide][0] = true_divide_back0\n",
    "BACK_FUNCS[np.true_divide][1] = true_divide_back1\n",
    "BACK_FUNCS[np.maximum][0] = maximum_back0\n",
    "BACK_FUNCS[np.maximum][1] = maximum_back1\n",
    "BACK_FUNCS[np.sum][0] = sum_back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829a951a-dd58-4ecf-a40c-32ada4bfe009",
   "metadata": {},
   "source": [
    "### Practice implementing a forward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6e956c9a-8459-424e-a085-f3f28970ca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(x: Tensor) -> Tensor:\n",
    "    t = tensor(np.log(x.array), requires_grad=True)\n",
    "    t.recipe = Recipe(\n",
    "        func=np.log,\n",
    "        args=x,\n",
    "        kwargs=None,\n",
    "        parents={0: x}\n",
    "    )\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be410f8d-c338-424c-8f03-dd664f488c79",
   "metadata": {},
   "source": [
    "### Generalized forward function wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0805f659-1e5c-4065-b1c2-2c901295f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_forward_fn(inner_func: Callable[[Arr], Arr]) -> Callable[[Tensor], Tensor]:\n",
    "    def func(*args, **kwargs):\n",
    "        np_args = tuple(arg.array for arg in args)\n",
    "        np_kwargs = {key: arg.array for key, arg in kwargs}\n",
    "        t = tensor(inner_func(*np_args, **np_kwargs), requires_grad=True)\n",
    "        t.recipe = Recipe(\n",
    "            func=inner_func,\n",
    "            args=np_args,\n",
    "            kwargs=np_kwargs,\n",
    "            parents={i: arg for i, arg in enumerate(args)}\n",
    "        )\n",
    "        return t\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1ff534-c257-49d0-b66f-66e23280551d",
   "metadata": {},
   "source": [
    "### Forward functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "fa5fb591-f663-4a69-912d-d8ff9f960bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _matmul2d():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b91714e7-0b5c-4386-b48d-d77df77feff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = wrap_forward_fn(np.log)\n",
    "add = wrap_forward_fn(np.add)\n",
    "subtract = wrap_forward_fn(np.subtract)\n",
    "multiply = wrap_forward_fn(np.multiply)\n",
    "true_divide = wrap_forward_fn(np.true_divide)\n",
    "sum = wrap_forward_fn(np.sum)\n",
    "maximum = wrap_forward_fn(np.maximum)\n",
    "matmul_2d = wrap_forward_fn(_matmul2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1028468f-b8a3-4268-8057-02ecf20b9e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x: Tensor) -> Tensor:\n",
    "    return maximum(x, np.zeros_like(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea875121-e8dc-44e0-94f3-6da2c9382bec",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "615862fe-e6cb-4563-b348-1a44b6c19528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topological_sort(root: Tensor) -> List[Tensor]:\n",
    "    # first do a DFS to get nodes with no parents and also a mapping from a node to its children\n",
    "    node_to_num_remaining_parents = {}\n",
    "    node_to_children = defaultdict(list)\n",
    "    def scan(node: Tensor):\n",
    "        if node.recipe is None:\n",
    "            node_to_num_remaining_parents[node] = 0\n",
    "            return\n",
    "        node_to_num_remaining_parents[node] = len(node.recipe.parents)\n",
    "        for parent in node.recipe.parents.values():\n",
    "            scan(parent)\n",
    "            node_to_children[parent].append(node)\n",
    "\n",
    "    scan(root)\n",
    "\n",
    "    # then iteratively add to the topological order from nodes with no remaining parents\n",
    "    topological_order = []\n",
    "    visited = set()\n",
    "\n",
    "    nodes_with_no_remaining_parents = [\n",
    "        node\n",
    "        for node, num_remaining_parents in node_to_num_remaining_parents.items()\n",
    "        if num_remaining_parents == 0\n",
    "    ]\n",
    "    \n",
    "    while len(nodes_with_no_remaining_parents) > 0:\n",
    "        cur_node = nodes_with_no_remaining_parents.pop()\n",
    "        topological_order.append(cur_node)\n",
    "        visited.add(cur_node)\n",
    "        for child in node_to_children[cur_node]:\n",
    "            if child in visited:\n",
    "                raise AssertionError(\"Found a cycle in the DAG\")\n",
    "            if child not in visited:\n",
    "                node_to_num_remaining_parents[child] -= 1\n",
    "                if node_to_num_remaining_parents[child] == 0:\n",
    "                    nodes_with_no_remaining_parents.append(child)\n",
    "                \n",
    "    return topological_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "94ea44f1-454f-47f8-b723-a25cfc54c5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(t: Tensor, end_grad: Optional[Tensor]):\n",
    "    # do a topological sort.\n",
    "    # visit the nodes in order and for each node call the right backward function\n",
    "    # what is end grad for? it's the last grad_out -- if it's none just make it all ones.\n",
    "    topologically_sorted_computational_graph = topological_sort(t)\n",
    "    if end_grad is None:\n",
    "        end_grad = np.ones_like(t.array)\n",
    "    t.grad = end_grad\n",
    "    \n",
    "    for cur_tensor in topologically_sorted_computational_graph[::-1]:\n",
    "        if cur_tensor.is_leaf:\n",
    "            continue\n",
    "        for cur_tensor_parent_index, cur_tensor_parent in cur_tensor.recipe.parents.items():\n",
    "            print(cur_tensor.grad)\n",
    "            print(cur_tensor.recipe.args)\n",
    "            print(cur_tensor.recipe.kwargs)\n",
    "            print(BACK_FUNCS[cur_tensor.recipe.func][cur_tensor_parent_index])\n",
    "            cur_tensor_parent.grad += BACK_FUNCS[cur_tensor.recipe.func][cur_tensor_parent_index](\n",
    "                cur_tensor.grad,\n",
    "                cur_tensor.array,\n",
    "                *cur_tensor.recipe.args,\n",
    "                **cur_tensor.recipe.kwargs,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0544604e-2a86-41ad-9547-559bbfda38e3",
   "metadata": {},
   "source": [
    "## Defining nn.Parameter and nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "c9de02d7-dadc-46d4-8a86-5d7e5a0cbb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter(Tensor):\n",
    "    def __init__(self, t: Tensor, requires_grad=True):\n",
    "        return super().__init__(t.array, requires_grad=requires_grad)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Parameter containing: {super().__repr__()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "71ee3dd3-3df6-462b-8057-f7508e765936",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    _modules: Dict[str, \"Module\"]\n",
    "    _parameters: Dict[str, Parameter]\n",
    "\n",
    "    def __init__(self):\n",
    "        self._modules = {}\n",
    "        self._parameters = {}\n",
    "\n",
    "    def modules(self):\n",
    "        '''Return the direct child modules of this module.'''\n",
    "        return self.__dict__[\"_modules\"].values()\n",
    "\n",
    "    def parameters(self, recurse: bool = True) -> Iterator[Parameter]:\n",
    "        '''\n",
    "        Return an iterator over Module parameters.\n",
    "\n",
    "        recurse: if True, the iterator includes parameters of submodules, recursively.\n",
    "        '''\n",
    "        def param_iter():\n",
    "            for param in self._parameters.values():\n",
    "                yield param\n",
    "\n",
    "            if recurse:\n",
    "                for submodule in self._modules.values():\n",
    "                    for param in submodule.parameters(recurse=True):\n",
    "                        yield param\n",
    "\n",
    "        return param_iter()\n",
    "\n",
    "    def __setattr__(self, key: str, val: Any) -> None:\n",
    "        '''\n",
    "        If val is a Parameter or Module, store it in the appropriate _parameters or _modules dict.\n",
    "        Otherwise, call __setattr__ from the superclass.\n",
    "        '''\n",
    "        if isinstance(val, Parameter):\n",
    "            self.__dict__[\"_parameters\"][key] = val\n",
    "        elif isinstance(val, Module):\n",
    "            self.__dict__[\"_modules\"][key] = val\n",
    "        else:\n",
    "            super().__setattr__(key, val)\n",
    "\n",
    "    def __getattr__(self, key: str) -> Union[Parameter, \"Module\"]:\n",
    "        '''\n",
    "        If key is in _parameters or _modules, return the corresponding value.\n",
    "        Otherwise, raise KeyError.\n",
    "        '''\n",
    "        if key in self.__dict__[\"_parameters\"].keys():\n",
    "            return self.__dict__[\"_parameters\"][key]\n",
    "        elif key in self.__dict__[\"_modules\"].keys():\n",
    "            return self.__dict__[\"_modules\"][key]\n",
    "        else:\n",
    "            raise KeyError(\"key not in submodules or parameters\")\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError(\"Subclasses must implement forward!\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        def _indent(s_, numSpaces):\n",
    "            return re.sub(\"\\n\", \"\\n\" + (\" \" * numSpaces), s_)\n",
    "        lines = [f\"({key}): {_indent(repr(module), 2)}\" for key, module in self._modules.items()]\n",
    "        return \"\".join([\n",
    "            self.__class__.__name__ + \"(\",\n",
    "            \"\\n  \" + \"\\n  \".join(lines) + \"\\n\" if lines else \"\", \")\"\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b798732-91d7-454e-a6c6-1e11138c00cd",
   "metadata": {},
   "source": [
    "### Defining Linear and ReLU modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "9524d523-12a0-4204-bf0f-62a68223abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # TODO: fix initialization\n",
    "        # TODO: add bias\n",
    "        self.weight = Parameter(tensor(np.zeros((in_features, out_features))))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return matmul2d(self.weight, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "34eb6587-79f6-4a2e-b593-9d2b8532bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "8d4cfca3-855b-481b-8082-46ab25b08162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCIFAR10MLP(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = Linear(3072, 200)\n",
    "        self.relu1 = ReLU()\n",
    "        self.linear2 = Linear(200, 200)\n",
    "        self.relu2 = ReLU()\n",
    "        self.output = Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.linear1(x))\n",
    "        x = self.relu2(self.linear2(x))\n",
    "        x = self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "a1fe4313-d191-4d5c-bb62-345edd11a720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(SimpleCIFAR10MLP().parameters(recurse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8436c77c-450d-47fa-b8b0-1c751141698b",
   "metadata": {},
   "source": [
    "## Defining our own XE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62384fe-dbfd-41af-bb51-61ce1bd2ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits: Tensor, true_labels: Tensor) -> Tensor:\n",
    "    # TODO: finish this\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eda0b7a-b31c-4110-9de1-706e19d8a605",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06db8993-f53b-4da6-8989-27ac4a1a5d7e",
   "metadata": {},
   "source": [
    "### Calling Backward Functions directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8aa23dd0-66fc-4112-a409-3c4b5cc9fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2])\n",
    "b = np.array([3, 4])\n",
    "c = np.array([5, 3])\n",
    "d = np.add(a, b)\n",
    "e = np.log(c)\n",
    "f = np.multiply(d, e)\n",
    "g = np.sum(f)\n",
    "\n",
    "dg_dg = np.ones_like(g)\n",
    "dg_df = sum_back(dg_dg, g, f)\n",
    "dg_de = multiply_back1(dg_df, f, d, e)\n",
    "dg_dd = multiply_back0(dg_df, f, d, e)\n",
    "dg_dc = log_back(dg_de, e, c)\n",
    "dg_db = add_back1(dg_dd, d, a, b)\n",
    "dg_da = add_back0(dg_dd, d, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b467c3ce-3b41-45a8-8993-a48822157396",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_g = g.copy()\n",
    "a = np.array([1, 2])\n",
    "b = np.array([3, 4])\n",
    "c = np.array([5.01, 3])\n",
    "d = np.add(a, b)\n",
    "e = np.log(c)\n",
    "f = np.multiply(d, e)\n",
    "g = np.sum(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "931aaac2-90e6-4bab-9383-3df64b7adc75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00799201065069255"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g - old_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "13099632-b3cd-4e80-bcb9-6084e492d15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8, 2. ])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg_dc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d438ecfb-ca56-4090-8dd9-06c61a2a4f79",
   "metadata": {},
   "source": [
    "### Forward Functions and Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "3bc2d8a6-d046-49d1-b3e2-039d058250b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "(array([6.437752, 6.591674], dtype=float32),)\n",
      "{}\n",
      "<function sum_back at 0x13d0ecd60>\n",
      "1.0 [0]\n",
      "[1. 1.]\n",
      "(array([4, 6]), array([1.609438 , 1.0986123], dtype=float32))\n",
      "{}\n",
      "<function multiply_back0 at 0x13d3c0ea0>\n",
      "[1. 1.]\n",
      "(array([4, 6]), array([1.609438 , 1.0986123], dtype=float32))\n",
      "{}\n",
      "<function multiply_back1 at 0x13d326980>\n",
      "[1.609438  1.0986123]\n",
      "(array([1, 2]), array([3, 4]))\n",
      "{}\n",
      "<function add_back0 at 0x13dcc6de0>\n",
      "[1.609438  1.0986123]\n",
      "(array([1, 2]), array([3, 4]))\n",
      "{}\n",
      "<function add_back1 at 0x13dcc7560>\n",
      "[4. 6.]\n",
      "(array([5, 3]),)\n",
      "{}\n",
      "<function log_back at 0x13dcc7100>\n"
     ]
    }
   ],
   "source": [
    "a = tensor([1, 2])\n",
    "b = tensor([3, 4])\n",
    "c = tensor([5, 3])\n",
    "d = add(a, b)\n",
    "e = log(c)\n",
    "f = multiply(d, e)\n",
    "g = sum(f)\n",
    "\n",
    "g.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc225fa-ecf8-46d7-9969-72caa04452f2",
   "metadata": {},
   "source": [
    "## Defining a basic training + testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5a3d39-7c6b-4809-a18c-cb1131acafc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement\n",
    "class SGD:\n",
    "    def __init__(self, params: Iterable[Parameter], lr: float):\n",
    "        pass\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def step(self) -> None:\n",
    "        pass\n",
    "\n",
    "\n",
    "def train(model: MLP, train_loader: DataLoader, optimizer: SGD, epoch: int, train_loss_list: Optional[list] = None):\n",
    "    pass\n",
    "\n",
    "def test(model: MLP, test_loader: DataLoader, test_loss_list: Optional[list] = None):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37989076-d98c-4263-bb3d-4f94344f9139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9eeb7872-fe33-4f93-ab82-93bdf678d476",
   "metadata": {},
   "source": [
    "## Pulling in CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "20736c97-7b58-440d-97a3-ce7207f03787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "1fd651b4-b4cf-4cb1-a9e1-39ed49c738bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_batch_1 = unpickle('data/cifar-10-batches-py/data_batch_1')\n",
    "data = data_batch_1[b'data']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
